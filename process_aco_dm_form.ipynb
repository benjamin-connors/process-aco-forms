{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b5dc376-67f4-4f82-b597-ec63e664a761",
   "metadata": {},
   "source": [
    "### process_aco_dm_form.ipynb\n",
    "The following script is used to process incoming device_magic forms for ACO Snow Surveys.\n",
    "\n",
    "It is scripted to automatically parse data for multiple survey locations (Cruickshank, Englishman, Tsitika, Metro_Van) and perform the following operations separately for each:\n",
    "\n",
    "1. Extract and rename desired columns from the DM form.\n",
    "2. Generate UTM coordinates for each sample, using provided GNSS data.\n",
    "3. Output a summary report for each survey location that includes summary statistics for each plot_id.\n",
    "4. Output a detailed spreadsheet for each survey location that includes XXX.\n",
    "5. Output a less-detailed spreadsheet that contains data necessary for use in the XXX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5d84777f-6559-4741-b086-7e77a858fc37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fdda80-869d-42a9-b32e-313d6270a544",
   "metadata": {
    "tags": []
   },
   "source": [
    "**USER INPUTS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d11a64d8-be76-4a0b-9312-11296d2505fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input flight number (AUTOMATE?)\n",
    "flt_no = '1'\n",
    "\n",
    "# provide target .csv file for DM form\n",
    "path = r\"G:\\ACO\\2024\"\n",
    "file = \"CRU_24_P01.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d92f954-2bfe-496f-be43-4b1974bf816a",
   "metadata": {},
   "source": [
    "**CODE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8903018d-e691-4d22-8025-7b925a711b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read file\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# set strings for each study area\n",
    "study_area = ['Cruickshank', 'Englishman', 'Metro Vancouver', 'Russell Creek']\n",
    "study_area_abr = ['CRU', 'EGM' , 'MV', 'TSI']\n",
    "\n",
    " # select columns to keep and new column names\n",
    "cols2keep = np.array([\"Survey_Start_Time\",\n",
    "      \"username\", \n",
    "      \"Study_Area\", \n",
    "      \"Other_Study_Area\", \n",
    "      \"User_s_\", \n",
    "      \"Plot_ID\", \n",
    "      \"Tube_Name\", \n",
    "      \"Pre_Survey_Notes\", \n",
    "      \"GNSS_Used_\", \n",
    "      \"GNSS_Setup\", \n",
    "      \"Other_GNSS_Setup\",\n",
    "      \"GNSS_Status\",\n",
    "      \"GNSS_Height_Rover_to_Snow__cm_\", \n",
    "      \"Tare_Weight__g_\",\n",
    "      \"Tare_Weight__cm_\", \n",
    "      \"Type_of_Plot\", \n",
    "      \"Point_Observation.Cardinal_Direction\", \n",
    "      \"Point_Observation.Distance_From_Centre__m_\",\n",
    "      \"Point_Observation.Custom_Distance_From_Centre__m_\",\n",
    "      \"Point_Observation.Plot_Features\", \n",
    "      \"Point_Observation.Sample_Type\", \n",
    "      \"Point_Observation.Depth__cm_\", \n",
    "      \"Point_Observation.Depth__cm_:timestamp\", \n",
    "      \"Point_Observation.Depth_Final__cm_\",\n",
    "      \"Point_Observation.Core_Length__cm_\", \n",
    "      \"Point_Observation.Plug__cm_\", \n",
    "      \"Point_Observation.SWE_cm\",\n",
    "      \"Point_Observation.Mass___Tube__g_\",\n",
    "      \"Point_Observation.Multi_Part_Core_\",\n",
    "      \"Point_Observation.Multi_Part_Core_Section_Number\", \n",
    "      \"Point_Observation.Additional_Measurements.Multi_Part_Core_Section_Number_copy\",\n",
    "      \"Point_Observation.Additional_Measurements.New_Depth__cm_\", \n",
    "      \"Point_Observation.Additional_Measurements.New_Depth__cm_:timestamp\", \n",
    "      \"Point_Observation.Additional_Measurements.Core_Section_Length__cm_\",\n",
    "      \"Point_Observation.Additional_Measurements.Plug_\", \n",
    "      \"Point_Observation.Additional_Measurements.SWE\",\n",
    "      \"Point_Observation.Additional_Measurements.Mass___Tube\", \n",
    "      \"Point_Observation.Core_Length_Final__cm_\",\n",
    "      \"Point_Observation.Core_Features\", \n",
    "      \"Point_Observation.Depth_of_Saturation\", \n",
    "      \"Point_Observation.Mass_Final__g_\", \n",
    "      \"Point_Observation.SWE_Final__cm_\",    \n",
    "      \"Point_Observation.Depth_Max\",\n",
    "      \"Point_Observation.Retrieval____\", \n",
    "      \"Point_Observation.SWE__cm_\", \n",
    "      \"Point_Observation.Density\", \n",
    "      \"Point_Observation.Density_MetroVan\",\n",
    "      \"Point_Observation.Sample_Rating\", \n",
    "      \"Point_Observation.Point_Observation_Notes\", \n",
    "      \"Snow_Pit_Measurement.Distance_from_centre_of_plot__m_\", \n",
    "      \"Snow_Pit_Measurement.Depth_above_ground__cm_\", \n",
    "      \"Snow_Pit_Measurement.Temperature___C_\",\n",
    "      \"Snow_Pit_Measurement.Density_Notes\", \n",
    "      \"Survey_End_TIme\"])\n",
    "\n",
    "new_colnames = np.array([\"plot_datetime\",\n",
    "    \"user_name\",\n",
    "    \"study_area\",\n",
    "    \"other_study_area\",\n",
    "    \"users\",\n",
    "    \"plot_id\",\n",
    "    \"tube_name\",\n",
    "    \"pre_survey_notes\",\n",
    "    \"gnss_unit\",\n",
    "    \"gnss_setup\",\n",
    "    \"other_gnss_setup\",\n",
    "    \"gnss_status\",\n",
    "    \"rover_height\",\n",
    "    \"tare_weight_g\",\n",
    "    \"tare_weight_cm\",\n",
    "    \"plot_type\",\n",
    "    \"cardinal\",\n",
    "    \"distance_m\",\n",
    "    \"custom_distance\",\n",
    "    \"plot_features\",\n",
    "    \"sample_type\",\n",
    "    \"depth_cm\",\n",
    "    \"depth_timestamp\",\n",
    "    \"depth_final_cm\",\n",
    "    \"core_length_cm\",\n",
    "    \"plug_cm\",\n",
    "    \"swe_cm\",\n",
    "    \"mass_tube_g_drop\",\n",
    "    \"multi_core\",\n",
    "    \"multi_part_core_num\",\n",
    "    \"multi_core_num_copy\",\n",
    "    \"depth_cm_new\",\n",
    "    \"depth_cm_timestamp_new\",\n",
    "    \"core_section_length_cm\",\n",
    "    \"plug_cm_copy\",\n",
    "    \"core_swe\",\n",
    "    \"mass_tube_g_new\",\n",
    "    \"core_length_final\",\n",
    "    \"core_features\",\n",
    "    \"depth_of_saturation\",\n",
    "    \"mass_final_g\",\n",
    "    \"swe_final_cm\",\n",
    "    \"depth_max\",\n",
    "    \"retrieval\",\n",
    "    \"swe_cm_drop\",\n",
    "    \"density_drop\",\n",
    "    \"density\",\n",
    "    \"sample_rating\",\n",
    "    \"notes\",\n",
    "    \"snow_pit_distance_from_centre\",\n",
    "    \"snow_pit_depth_above_ground\",\n",
    "    \"snow_pit_temperature\",\n",
    "    \"snow_pit_density\",\n",
    "    \"survey_end\"])\n",
    "\n",
    "# find columns to keep that exist in spreadsheet\n",
    "ix = np.in1d(cols2keep, df.columns, assume_unique=True)\n",
    "\n",
    " # extract and rename these columns\n",
    "df = df[cols2keep[ix]].set_axis(new_colnames[ix], axis='columns')\n",
    "\n",
    "#  add ACO flight no.\n",
    "df.insert(0, 'aco_flight_number', str(flt_no))\n",
    "\n",
    "# fill nan depths with 0\n",
    "df[['depth_final_cm', 'depth_max']] = df[['depth_final_cm', 'depth_max']].fillna(value=0)\n",
    "\n",
    "# calculate snow depth (for multi-cores?)\n",
    "df.insert(df.columns.get_loc('multi_core'), 'snow_depth',  df['depth_final_cm'] + df['depth_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a8c7a951-df74-4f9d-a301-325f0fd8c2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['aco_flight_number', 'plot_datetime', 'user_name', 'study_area',\n",
      "       'other_study_area', 'users', 'plot_id', 'tube_name', 'pre_survey_notes',\n",
      "       'gnss_unit', 'gnss_setup', 'other_gnss_setup', 'gnss_status',\n",
      "       'rover_height', 'tare_weight_g', 'plot_type', 'cardinal', 'distance_m',\n",
      "       'custom_distance', 'plot_features', 'sample_type', 'depth_cm',\n",
      "       'depth_timestamp', 'depth_final_cm', 'core_length_cm', 'plug_cm',\n",
      "       'mass_tube_g_drop', 'snow_depth', 'multi_core', 'multi_part_core_num',\n",
      "       'multi_core_num_copy', 'depth_cm_new', 'depth_cm_timestamp_new',\n",
      "       'core_section_length_cm', 'plug_cm_copy', 'mass_tube_g_new',\n",
      "       'core_length_final', 'core_features', 'depth_of_saturation',\n",
      "       'mass_final_g', 'depth_max', 'retrieval', 'swe_cm_drop', 'density_drop',\n",
      "       'sample_rating', 'notes', 'snow_pit_distance_from_centre',\n",
      "       'snow_pit_depth_above_ground', 'snow_pit_temperature',\n",
      "       'snow_pit_density', 'survey_end'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# NEED TO KNOW WHAT VARABLES ARE TO BE INCLUDED (SOME ARE READ ABOVE AND THEN DROPPED HERE...? a MV VS. OTHER SITE THING?)\n",
    "\n",
    "# df = new_df.drop(columns=['submissionid', \n",
    "#                                 'submissiondatetime', \n",
    "#                                 'deviceid', \n",
    "#                                 'user_name', \n",
    "#                                 'Sampling_Design',\n",
    "#                                 'Point_Observation.Abbreviations', \n",
    "#                                 'Point_Observation.Picture', \n",
    "#                                 'Point_Observation.Number_of_probe_extensions_used_',\n",
    "#                                 'mass_tube_g_drop',\n",
    "#                                 'mass_final_g',\n",
    "#                                 'swe_cm_drop',\n",
    "#                                 'density_drop'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f2f444b5-3652-4f31-8dd4-53dd50129a93",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['density', 'swe_cm'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m df_area\u001b[38;5;241m.\u001b[39mto_csv(study_area_abr[ix] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_trip\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(flt_no) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_DMform_clean.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# get summary statistics\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m df_area_summary \u001b[38;5;241m=\u001b[39m df_area\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maco_flight_number\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplot_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39magg({\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnow_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdensity\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswe_cm\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#check if directory exists\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# if not os.path.isdir(path + r\"\\Cruickshank\\4_field_data\\plots\\working\\P\" + str(flt_no)):\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#if the folder path is not presnet, then create it\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# os.makedirs(path + r\"\\Cruickshank\\4_field_data\\plots\\working\\P\" + str(flt_no))\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#Export the filtered summary stats to created folder\u001b[39;00m\n\u001b[0;32m     23\u001b[0m df_area_summary\u001b[38;5;241m.\u001b[39mto_csv(study_area_abr[ix] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_trip\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(flt_no) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_DMform_summary.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\chrl\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:1432\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1429\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[0;32m   1431\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m-> 1432\u001b[0m result \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39magg()\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\chrl\\Lib\\site-packages\\pandas\\core\\apply.py:190\u001b[0m, in \u001b[0;36mApply.agg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_dict_like()\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\chrl\\Lib\\site-packages\\pandas\\core\\apply.py:423\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_or_apply_dict_like(op_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\chrl\\Lib\\site-packages\\pandas\\core\\apply.py:1608\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[1;34m(self, op_name)\u001b[0m\n\u001b[0;32m   1603\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[0;32m   1605\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[0;32m   1606\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1607\u001b[0m ):\n\u001b[1;32m-> 1608\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dict_like(\n\u001b[0;32m   1609\u001b[0m         op_name, selected_obj, selection, kwargs\n\u001b[0;32m   1610\u001b[0m     )\n\u001b[0;32m   1611\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[0;32m   1612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\chrl\\Lib\\site-packages\\pandas\\core\\apply.py:462\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[1;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m is_groupby \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001b[0;32m    461\u001b[0m func \u001b[38;5;241m=\u001b[39m cast(AggFuncTypeDict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[1;32m--> 462\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_dictlike_arg(op_name, selected_obj, func)\n\u001b[0;32m    464\u001b[0m is_non_unique_col \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    465\u001b[0m     selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(selected_obj\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    467\u001b[0m )\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\chrl\\Lib\\site-packages\\pandas\\core\\apply.py:663\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[1;34m(self, how, obj, func)\u001b[0m\n\u001b[0;32m    661\u001b[0m     cols \u001b[38;5;241m=\u001b[39m Index(\u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys()))\u001b[38;5;241m.\u001b[39mdifference(obj\u001b[38;5;241m.\u001b[39mcolumns, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    665\u001b[0m aggregator_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Column(s) ['density', 'swe_cm'] do not exist\""
     ]
    }
   ],
   "source": [
    "# loop study areas and export cleaned spreadsheet and summary spreadheet for each\n",
    "for ii in df[\"study_area\"].unique():\n",
    "    ix = study_area.index(ii)\n",
    "    \n",
    "    # get data for study area\n",
    "    df_area = df[df['study_area'] == ii]\n",
    "    \n",
    "    # export cleaned spreadsheet\n",
    "    df_area.to_csv(study_area_abr[ix] + '_trip' + str(flt_no) + '_DMform_clean.csv', index=False)\n",
    "    \n",
    "    # get summary statistics\n",
    "    df_area_summary = df_area.groupby(['aco_flight_number','plot_id']).agg({\n",
    "    \"snow_depth\": [\"mean\", \"median\", \"std\", \"count\"],\n",
    "    \"density\": [\"mean\", \"median\", \"std\", \"count\"],\n",
    "    \"swe_cm\": [\"mean\", \"median\", \"std\", \"count\"]})\n",
    "    \n",
    "    #check if directory exists\n",
    "    # if not os.path.isdir(path + r\"\\Cruickshank\\4_field_data\\plots\\working\\P\" + str(flt_no)):\n",
    "    #if the folder path is not presnet, then create it\n",
    "    # os.makedirs(path + r\"\\Cruickshank\\4_field_data\\plots\\working\\P\" + str(flt_no))\n",
    "    \n",
    "    #Export the filtered summary stats to created folder\n",
    "    df_area_summary.to_csv(study_area_abr[ix] + '_trip' + str(flt_no) + '_DMform_summary.csv', index=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chrl]",
   "language": "python",
   "name": "conda-env-chrl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
